{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313d427f-8151-4f79-b308-b2b28514460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tabpfn import TabPFNClassifier  \n",
    "from functools import partial\n",
    "import tabpfn.encoders as encoders\n",
    "from tabpfn.scripts.transformer_prediction_interface import transformer_predict, get_params_from_config, load_model_workflow_my\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn  # 确保引入 nn 模块\n",
    "import torch.optim as optim  # 确保引入 optim 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6878ee7-0667-4963-817f-538f50bcde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train = pd.read_csv('/root/autodl-fs/data/train_revise+45缩减到100特征 数量1000个 去掉三列和Name.csv')  \n",
    "\n",
    "# 分离特征和标签\n",
    "X = train.drop(['senolytic'], axis=1).values\n",
    "y = train['senolytic'].values\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用 SMOTE 对训练集进行过采样\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 确保数据加载时的目标标签为 Long 类型\n",
    "X_train_tensor = torch.tensor(X_train_balanced, dtype=torch.float32).cuda()\n",
    "y_train_tensor = torch.tensor(y_train_balanced, dtype=torch.long).cuda()  # 确保为 long 类型\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).cuda()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).cuda()  # 确保为 long 类型\n",
    "\n",
    "\n",
    "# 构造 DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9e4f52-738e-493a-b752-327fd068e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /root/autodl-fs/Sq-TabPFN/tabpfn/smote+sq_models_diff/prior_diff_real_checkpoint_n_1_epoch_110.cpkt\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 TabPFNClassifier 模型\n",
    "model = TabPFNClassifier(device='cuda', model_file=\"/root/autodl-fs/Sq-TabPFN/tabpfn/smote+sq_models_diff/prior_diff_real_checkpoint_n_1_epoch_110.cpkt\")\n",
    "transformer_model = model.model[2]  # 获取模型的 transformer 模块\n",
    "\n",
    "# 加载保存的模型状态\n",
    "checkpoint_path = \"/root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调解码器模型/prior_diff_real_checkpoint_n_1_epoch_1.cpkt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# 加载模型权重\n",
    "transformer_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "transformer_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af761211-1a18-4d81-a523-842c21399457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结解码器参数\n",
    "for name, param in transformer_model.decoder.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74282e98-c5d2-4344-a4c7-affcc0475a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 设置优化器和损失函数\\ncriterion = nn.BCEWithLogitsLoss()  # 二分类问题的损失函数\\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, transformer_model.parameters()), lr=1e-4)\\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 设置优化器和损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, transformer_model.parameters()), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "'''\n",
    "# 设置优化器和损失函数\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类问题的损失函数\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, transformer_model.parameters()), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8863e1-c46c-4336-aa95-834d423a17f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing encoder layer 1/12\n",
      "Layer 1, Epoch 1/5 - Train Loss: 0.7380, Validation Loss: 0.6321\n",
      "New best model saved at epoch 1 with Validation Loss: 0.6321 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_1_epoch_1.cpkt\n",
      "Layer 1, Epoch 2/5 - Train Loss: 0.6411, Validation Loss: 0.5463\n",
      "New best model saved at epoch 2 with Validation Loss: 0.5463 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_1_epoch_2.cpkt\n",
      "Layer 1, Epoch 3/5 - Train Loss: 0.6305, Validation Loss: 0.7773\n",
      "Layer 1, Epoch 4/5 - Train Loss: 0.6404, Validation Loss: 0.7169\n",
      "Layer 1, Epoch 5/5 - Train Loss: 0.6099, Validation Loss: 0.8282\n",
      "Unfreezing encoder layer 2/12\n",
      "Layer 2, Epoch 1/5 - Train Loss: 0.6595, Validation Loss: 0.7568\n",
      "Layer 2, Epoch 2/5 - Train Loss: 0.5862, Validation Loss: 0.4234\n",
      "New best model saved at epoch 2 with Validation Loss: 0.4234 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_2_epoch_2.cpkt\n",
      "Layer 2, Epoch 3/5 - Train Loss: 0.5955, Validation Loss: 0.5897\n",
      "Layer 2, Epoch 4/5 - Train Loss: 0.6119, Validation Loss: 0.4241\n",
      "Layer 2, Epoch 5/5 - Train Loss: 0.5933, Validation Loss: 0.5911\n",
      "Unfreezing encoder layer 3/12\n",
      "Layer 3, Epoch 1/5 - Train Loss: 0.5782, Validation Loss: 0.5267\n",
      "Layer 3, Epoch 2/5 - Train Loss: 0.6147, Validation Loss: 0.5070\n",
      "Layer 3, Epoch 3/5 - Train Loss: 0.5676, Validation Loss: 0.5872\n",
      "Layer 3, Epoch 4/5 - Train Loss: 0.5500, Validation Loss: 0.5756\n",
      "Layer 3, Epoch 5/5 - Train Loss: 0.5465, Validation Loss: 0.4743\n",
      "Unfreezing encoder layer 4/12\n",
      "Layer 4, Epoch 1/5 - Train Loss: 0.5460, Validation Loss: 0.7625\n",
      "Layer 4, Epoch 2/5 - Train Loss: 0.5332, Validation Loss: 0.5129\n",
      "Layer 4, Epoch 3/5 - Train Loss: 0.5229, Validation Loss: 0.5545\n",
      "Layer 4, Epoch 4/5 - Train Loss: 0.5396, Validation Loss: 0.4354\n",
      "Layer 4, Epoch 5/5 - Train Loss: 0.5412, Validation Loss: 0.4129\n",
      "New best model saved at epoch 5 with Validation Loss: 0.4129 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_4_epoch_5.cpkt\n",
      "Unfreezing encoder layer 5/12\n",
      "Layer 5, Epoch 1/5 - Train Loss: 0.5519, Validation Loss: 0.4580\n",
      "Layer 5, Epoch 2/5 - Train Loss: 0.5447, Validation Loss: 0.6434\n",
      "Layer 5, Epoch 3/5 - Train Loss: 0.5258, Validation Loss: 0.5806\n",
      "Layer 5, Epoch 4/5 - Train Loss: 0.5071, Validation Loss: 0.5106\n",
      "Layer 5, Epoch 5/5 - Train Loss: 0.4785, Validation Loss: 0.5431\n",
      "Unfreezing encoder layer 6/12\n",
      "Layer 6, Epoch 1/5 - Train Loss: 0.5084, Validation Loss: 0.4578\n",
      "Layer 6, Epoch 2/5 - Train Loss: 0.4773, Validation Loss: 0.3980\n",
      "New best model saved at epoch 2 with Validation Loss: 0.3980 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_6_epoch_2.cpkt\n",
      "Layer 6, Epoch 3/5 - Train Loss: 0.4697, Validation Loss: 0.4840\n",
      "Layer 6, Epoch 4/5 - Train Loss: 0.4701, Validation Loss: 0.5227\n",
      "Layer 6, Epoch 5/5 - Train Loss: 0.4292, Validation Loss: 0.4656\n",
      "Unfreezing encoder layer 7/12\n",
      "Layer 7, Epoch 1/5 - Train Loss: 0.4345, Validation Loss: 0.5651\n",
      "Layer 7, Epoch 2/5 - Train Loss: 0.4145, Validation Loss: 0.4167\n",
      "Layer 7, Epoch 3/5 - Train Loss: 0.4177, Validation Loss: 0.4931\n",
      "Layer 7, Epoch 4/5 - Train Loss: 0.4384, Validation Loss: 0.7674\n",
      "Layer 7, Epoch 5/5 - Train Loss: 0.4058, Validation Loss: 0.5042\n",
      "Unfreezing encoder layer 8/12\n",
      "Layer 8, Epoch 1/5 - Train Loss: 0.3912, Validation Loss: 0.5279\n",
      "Layer 8, Epoch 2/5 - Train Loss: 0.3792, Validation Loss: 0.4247\n",
      "Layer 8, Epoch 3/5 - Train Loss: 0.3767, Validation Loss: 0.6867\n",
      "Layer 8, Epoch 4/5 - Train Loss: 0.3833, Validation Loss: 0.5718\n",
      "Layer 8, Epoch 5/5 - Train Loss: 0.3497, Validation Loss: 0.5028\n",
      "Unfreezing encoder layer 9/12\n",
      "Layer 9, Epoch 1/5 - Train Loss: 0.3545, Validation Loss: 0.4059\n",
      "Layer 9, Epoch 2/5 - Train Loss: 0.3590, Validation Loss: 0.3854\n",
      "New best model saved at epoch 2 with Validation Loss: 0.3854 to /root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_9_epoch_2.cpkt\n",
      "Layer 9, Epoch 3/5 - Train Loss: 0.3821, Validation Loss: 0.4922\n",
      "Layer 9, Epoch 4/5 - Train Loss: 0.3538, Validation Loss: 0.5259\n",
      "Layer 9, Epoch 5/5 - Train Loss: 0.3009, Validation Loss: 0.6153\n",
      "Unfreezing encoder layer 10/12\n",
      "Layer 10, Epoch 1/5 - Train Loss: 0.2918, Validation Loss: 0.4587\n",
      "Layer 10, Epoch 2/5 - Train Loss: 0.3420, Validation Loss: 0.7672\n",
      "Layer 10, Epoch 3/5 - Train Loss: 0.2988, Validation Loss: 0.5768\n",
      "Layer 10, Epoch 4/5 - Train Loss: 0.3041, Validation Loss: 0.6658\n",
      "Layer 10, Epoch 5/5 - Train Loss: 0.3001, Validation Loss: 0.4555\n",
      "Unfreezing encoder layer 11/12\n",
      "Layer 11, Epoch 1/5 - Train Loss: 0.2825, Validation Loss: 0.6063\n",
      "Layer 11, Epoch 2/5 - Train Loss: 0.2448, Validation Loss: 0.4892\n",
      "Layer 11, Epoch 3/5 - Train Loss: 0.2625, Validation Loss: 0.5209\n",
      "Layer 11, Epoch 4/5 - Train Loss: 0.2786, Validation Loss: 0.5723\n",
      "Layer 11, Epoch 5/5 - Train Loss: 0.2827, Validation Loss: 0.7211\n",
      "Unfreezing encoder layer 12/12\n",
      "Layer 12, Epoch 1/5 - Train Loss: 0.2618, Validation Loss: 0.5388\n",
      "Layer 12, Epoch 2/5 - Train Loss: 0.1917, Validation Loss: 0.8230\n",
      "Layer 12, Epoch 3/5 - Train Loss: 0.1917, Validation Loss: 0.5890\n",
      "Layer 12, Epoch 4/5 - Train Loss: 0.2403, Validation Loss: 0.8291\n",
      "Layer 12, Epoch 5/5 - Train Loss: 0.2154, Validation Loss: 0.7962\n"
     ]
    }
   ],
   "source": [
    "# 解冻 encoder 的所有参数\n",
    "for name, param in transformer_model.encoder.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 获取 transformer_encoder 中的层数\n",
    "num_encoder_layers = len(transformer_model.transformer_encoder.layers)\n",
    "\n",
    "# 设置保存路径\n",
    "save_dir = \"/root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型\"\n",
    "best_val_loss = float('inf')  # 用于记录最佳验证损失\n",
    "\n",
    "# 逐层解冻 transformer_encoder\n",
    "for i in range(num_encoder_layers):\n",
    "    print(f\"Unfreezing encoder layer {i+1}/{num_encoder_layers}\")\n",
    "    for name, param in transformer_model.transformer_encoder.layers[i].named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    single_eval_pos = 0  # 设置默认评估位置\n",
    "\n",
    "    # 在训练循环中\n",
    "    for epoch in range(5):  # 每层训练 5 个 epoch\n",
    "        transformer_model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # 训练阶段\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = transformer_model((x, y.float()), single_eval_pos=single_eval_pos).squeeze(1)\n",
    "            loss = criterion(outputs, y.long())  # 修正为 long 类型\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # 验证阶段\n",
    "        transformer_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x, y = batch\n",
    "                outputs = transformer_model((x, y.float()), single_eval_pos=single_eval_pos).squeeze(1)\n",
    "                loss = criterion(outputs, y.long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # 打印训练和验证损失\n",
    "        print(f\"Layer {i+1}, Epoch {epoch+1}/5 - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # 保存验证集损失最小的模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_path = f\"{save_dir}/prior_diff_real_checkpoint_n_{i+1}_epoch_{epoch+1}.cpkt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': transformer_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "            print(f\"New best model saved at epoch {epoch+1} with Validation Loss: {avg_val_loss:.4f} to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492447d8-3740-4024-8b74-6d1e9fc6f3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型权重\n",
    "checkpoint_path = \"/root/autodl-fs/Sq-TabPFN/tabpfn/增强微调模型/微调编码器模型/prior_diff_real_checkpoint_n_9_epoch_2.cpkt\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# 假设 transformer_model 是已经初始化的 TransformerModel\n",
    "model = transformer_model  # 使用同一个模型类实例\n",
    "model.load_state_dict(checkpoint['model_state_dict'])  # 加载权重\n",
    "model.cuda()  # 将模型移动到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db315554-83f5-4903-a107-7373b72a0ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8350\n",
      "Precision: 0.5000\n",
      "Recall: 0.6667\n",
      "F1 Score: 0.5714\n",
      "ROC AUC: 0.8100\n",
      "PR AUC: 0.5297\n"
     ]
    }
   ],
   "source": [
    "# 模型验证和指标计算\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []  # 用于 ROC 和 PR 曲线\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            src = (x, y.float())  # 构造符合模型输入格式的元组\n",
    "            outputs = model(src, single_eval_pos=0)\n",
    "            \n",
    "            # 获取预测类别和概率\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_prob.extend(outputs[:, 1].cpu().numpy())  # 假设二分类，取概率值\n",
    "\n",
    "    # 转换为 NumPy 数组\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    # 打印指标\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    return y_true, y_pred, y_prob, precision_vals, recall_vals, roc_auc, pr_auc\n",
    "\n",
    "# 评估模型性能\n",
    "y_true, y_pred, y_prob, precision_vals, recall_vals, roc_auc, pr_auc = evaluate_model(transformer_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82491149-dcb1-42b9-b9db-fd9aeff73785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
