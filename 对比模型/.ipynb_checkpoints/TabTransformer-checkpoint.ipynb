{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfa316f-55af-4fbf-a51a-f1810425cf4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TabTransformerConfig' from 'transformers' (/root/miniconda3/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabTransformerConfig, TabTransformerForSequenceClassification\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TabTransformerConfig' from 'transformers' (/root/miniconda3/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import TabTransformerConfig, TabTransformerForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a8a7c-def2-4b0c-a4f3-7aede0369a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train = pd.read_csv('/root/autodl-fs/data/train_revise+45缩减到100特征 数量1000个 去掉三列和Name.csv')\n",
    "\n",
    "# 分离特征和标签\n",
    "X = train.drop(['senolytic'], axis=1)\n",
    "y = train['senolytic']\n",
    "\n",
    "# 标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# 特征标准化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a0221-a713-4812-b057-a780f83b0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为Tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 配置TabTransformer模型\n",
    "config = TabTransformerConfig(\n",
    "    num_labels=2,\n",
    "    hidden_size=32,\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=64,\n",
    ")\n",
    "\n",
    "# 初始化TabTransformer模型\n",
    "model = TabTransformerForSequenceClassification(config)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a393fd-e0c4-49ac-9a0e-ffefffb8ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, train_loader, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "# 定义测试函数\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    y_pred, y_pred_proba, y_true = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            probs = outputs.logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
    "            predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(predictions)\n",
    "            y_pred_proba.extend(probs)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e1e38-5405-467e-a002-ce82ff479198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "train(model, train_loader, optimizer, epochs=10)\n",
    "\n",
    "# 评估模型\n",
    "y_true, y_pred, y_pred_proba = evaluate(model, test_loader)\n",
    "\n",
    "# 计算评价指标\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616787e8-fc39-41c9-9520-c8533668451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制ROC曲线\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fpr, tpr, label=f'TabTransformer (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a64ec-2573-407a-a1b4-b0cae524d0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 绘制PR曲线并计算AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall_vals, precision_vals, label=f'TabTransformer (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
